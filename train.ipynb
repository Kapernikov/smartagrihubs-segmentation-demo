{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook\n",
    "\n",
    "In this notebook, we will show how to train a UNet network to segment defects in hazelnut images from the MVTec Anomaly Detection [dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad).\n",
    "\n",
    "\n",
    "First, import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import namegenerator as namegen\n",
    "from models.model_functions import create_model, load_model\n",
    "from models.train_test.train import train_model\n",
    "from models.saving import (load_params, save_losses, save_model, save_params)\n",
    "from utils.utils import (create_metadata, save_metadata, create_color_map, create_category_dict)\n",
    "\n",
    "from processing.preprocessing import preprocess_data_from_images\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All configuration parameters are defined in `env.yaml` file. These parameters include information about the location of the dataset and results folders, the architecture of the network, and its training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "cfg = OmegaConf.load('configs/env.yaml')\n",
    "\n",
    "# used for inference only\n",
    "categ_dict = create_category_dict(cfg.MODEL.categories)\n",
    "color_map = create_color_map(cfg.MODEL.categories)\n",
    "\n",
    "print('Class categories', categ_dict)\n",
    "\n",
    "num_classes = len(cfg.MODEL.categories) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either load existing model and use it for transfer learning, or create a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.MODEL.model_name == '': \n",
    "    # generate a new model name\n",
    "    model_name = namegen.gen(separator='_')\n",
    "\n",
    "    model = create_model(eval(cfg.DATA.img_dims), num_classes, cfg.TRAINING.filters)\n",
    "else:\n",
    "    # use existing model with its proper name\n",
    "    model_name = cfg.MODEL.model_name\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    load_params(model_name)\n",
    "\n",
    "print(f'Model name is {model_name}')    \n",
    "    \n",
    "metadata = create_metadata(model_name)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load and preprocess images and masks from the dataset. Further we split it into training and testing sets. We will use only the training subset for training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET LOADING AND PREPROCESSING === #\n",
    "X, y = preprocess_data_from_images(data_path = cfg.DIRS.data, \n",
    "                                   shape = eval(cfg.DATA.img_dims),\n",
    "                                   categories = cfg.MODEL.categories)\n",
    "\n",
    "#=== TRAIN/TEST SPLIT === #\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=cfg.DATA.test_split,\n",
    "                                                    shuffle=cfg.TRAINING.shuffle,\n",
    "                                                    random_state=cfg.DATA.seed)\n",
    "\n",
    "print(f'Number of TRAIN images: {len(X_train)}')\n",
    "print(f'Number of TEST images: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(model, model_name, Xs=X_train, Ys=y_train, cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we save the model as well as loss function and metric evolution plots to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Saving model informations == #\n",
    "PATH_RESULTS = os.path.join(cfg.DIRS.results, model_name)\n",
    "PATH_LOG = os.path.join(cfg.DIRS.history, model_name)\n",
    "\n",
    "os.makedirs(PATH_LOG, exist_ok=True)\n",
    "\n",
    "save_losses(history, PATH_RESULTS)\n",
    "save_model(model, PATH_LOG)\n",
    "save_metadata(metadata, PATH_LOG)\n",
    "save_params(PATH_LOG, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the results in the folders provided in `env.yaml`. By default, the results are saved in `results` and model checkpoints are saved in `model_versioning` respectively.\n",
    "\n",
    "We show how to perform inference (make predictions) in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "655936058c9faa9280730ea635086effdc23d6308265a7763ad2c8854fe885a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
